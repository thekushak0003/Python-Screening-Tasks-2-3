# Research Plan: Evaluating Open Source Models for Student Competence Analysis

## Paragraph 1: Model Discovery and Evaluation Approach

My systematic approach to identifying relevant open source models will target three distinct categories that collectively address the multifaceted nature of student competence analysis in Python learning. First, I will evaluate **specialized code analysis models** including CodeT5+, CodeBERT, and Code Llama variants, focusing on their ability to parse Python syntax, understand semantic meaning, and identify logical inconsistencies in student submissions. Second, I will investigate **educational AI frameworks** such as the open-source components from Carnegie Mellon's CTAT (Cognitive Tutor Authoring Tools) and MIT's educational technology initiatives, which are specifically designed for adaptive learning scenarios. Third, I will assess **fine-tunable language models** like Mistral-7B, Llama-2, and Phi-3 that could be adapted for educational tasks through domain-specific training. My evaluation criteria will be weighted across four critical dimensions: **technical competence** (accuracy in code analysis, ability to identify misconceptions), **pedagogical appropriateness** (generation of scaffolded prompts, avoidance of direct solutions), **practical deployment feasibility** (computational requirements, inference speed, integration complexity), and **educational validation evidence** (peer-reviewed studies, documented learning outcomes). This multi-pronged approach ensures comprehensive coverage while maintaining focus on models that can realistically be implemented in educational environments with limited computational resources.

## Paragraph 2: Testing and Validation Framework

To rigorously test model applicability, I will develop a comprehensive validation framework using a curated dataset of authentic student Python submissions spanning three competency levels: novice (syntax and basic logic errors), intermediate (algorithmic thinking and data structure misconceptions), and advanced (optimization and design pattern issues). My testing methodology will involve **quantitative assessment** through automated metrics measuring prompt relevance, conceptual accuracy, and appropriate difficulty calibration, alongside **qualitative evaluation** by experienced Python educators who will rate generated prompts for pedagogical value, clarity, and learning scaffolding effectiveness. The validation process will include A/B testing different prompt engineering strategies, measuring model performance across diverse Python concepts (loops, functions, object-oriented programming, data structures), and conducting feasibility studies for real-time deployment scenarios. I will specifically test each model's ability to generate diagnostic questions that reveal student thinking processes, provide conceptual explanations without revealing solutions, and adapt prompt complexity based on demonstrated student competence levels. Success metrics will include inter-rater reliability among educator evaluators, student engagement scores from pilot testing, and measurable learning outcomes when prompts are used in controlled educational settings, ensuring that selected models not only demonstrate technical capability but also contribute meaningfully to student learning progression.
